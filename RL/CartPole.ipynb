{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "载入包以及配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import numpy as py\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import os\n",
    "\n",
    "## 如何建立网络估计reward \n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, agent_class=None, env_class=None, env_args=None):\n",
    "        self.agent_class = agent_class\n",
    "        self.if_off_policy = True\n",
    "\n",
    "        self.env_class = env_class\n",
    "        self.env_agrs = env_args\n",
    "        if env_args is None:\n",
    "            env_args = {'env_name': None, 'state_dim': None, 'action_dim': None, 'if_discrete': None}\n",
    "        self.env_name = env_args['env_name']\n",
    "        self.state_dim = env_args['state_dim']\n",
    "        self.action_dim = env_args['action_dim']\n",
    "        self.if_discrete = env_args['if_discrete']\n",
    "    ### 奖励函数建立\n",
    "        self.gamma = 0.99\n",
    "        self.reward_scale = 1.0\n",
    "    #### 训练\n",
    "        self.net_dims = (64,32)\n",
    "        self.batch_size = int(64)\n",
    "        self.horizon_len = int(512)\n",
    "        self.buffer_size = int(1e6)\n",
    "        self.repeat_times = 1\n",
    "        self.learning_rate = 6e-5  # 2 ** -14 ~= 6e-5\n",
    "        self.soft_update_tau = 5e-3  # 2 ** -8 ~= 5e-3\n",
    "        ### device\n",
    "        self.cwd = None\n",
    "        self.break_step = +np.inf\n",
    "        self.eval_times = int(32)\n",
    "        self.eval_per_step = int(10000)\n",
    "    \n",
    "    def init_before_training(self):\n",
    "        if self.cwd is None:\n",
    "            self.cwd = f'./{self.env_name}_{self.agent_class.__name[5:]}'\n",
    "        os.makedirs(self.cwd,exist_ok=True)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "建立Q 神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 建立QNet\n",
    "class QNet(nn.Module): \n",
    "    def __init__(self, n_hidden:int, n_dim: int, state_dim: int, action_dim: int,epls_rate):\n",
    "        super().__init__()\n",
    "        net_list = []\n",
    "        net_list.extend([nn.Linear(2,n_dim),nn.ReLU()])\n",
    "        for i in range (n_hidden):\n",
    "            net_list.extend([nn.Linear(n_dim,n_dim)],nn.ReLU())\n",
    "        net_list.append(nn.Linear(n_dim,1))\n",
    "        self.net = nn.Sequential(*net_list)\n",
    "        self.action_dim = action_dim\n",
    "        self.epls_rate = epls_rate\n",
    "\n",
    "    def forward(self, state:torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(state)\n",
    "    def get_action(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        if self.epls_rate < torch.rand(1):\n",
    "            torch.argmax(self.net(state),dim=1,keepdim=True )\n",
    "        else:\n",
    "            action = torch.randint(self.action_dim, size = (state.shape[0],1))\n",
    "        return action\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立环境以及智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gym_env_args(env, if_print: bool) -> dict:\n",
    "    if {'unwrapped', 'observation_space', 'action_space', 'spec'}.issubset(dir(env)):  # isinstance(env, gym.Env):\n",
    "        env_name = env.unwrapped.spec.id\n",
    "        state_shape = env.observation_space.shape\n",
    "        state_dim = state_shape[0] if len(state_shape) == 1 else state_shape  # sometimes state_dim is a list\n",
    "        if_discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "        action_dim = env.action_space.n if if_discrete else env.action_space.shape[0]\n",
    "    else:\n",
    "        env_name = env.env_name\n",
    "        state_dim = env.state_dim\n",
    "        action_dim = env.action_dim\n",
    "        if_discrete = env.if_discrete\n",
    "    env_args = {'env_name': env_name, 'state_dim': state_dim, 'action_dim': action_dim, 'if_discrete': if_discrete}\n",
    "    print(f\"env_args = {repr(env_args)}\") if if_print else None\n",
    "    return env_args\n",
    "\n",
    "def build_env(env_class = None, env_agrs = None):\n",
    "    if env_class.__module__ == 'gym_envs.registration':\n",
    "        assert '0.18.0' <= gym.__version__ <= '0.25.2'\n",
    "        env = env_class(id = env_agrs['env_name'])\n",
    "    for attr_str in ('env_name', 'state_dim', 'action_dim', 'if_discrete'):\n",
    "        setattr(env,attr_str,env_agrs[attr_str])\n",
    "    return env\n",
    "\n",
    "class AgentBase:\n",
    "    def __init__(self, net_dims, state_dim: int, action_dim: int, args: Config = Config()):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = args.gamma\n",
    "        self.batch_size = args.batch_size\n",
    "        self.repeat_times = args.repeat_times\n",
    "        self.reward_scale = args.reward_scale\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.soft_update_tau = args.soft_update_tau\n",
    "        self.last_state = None\n",
    "        self.device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        act_class = getattr(self, \"act_class\", None)\n",
    "        cri_class = getattr(self, \"cri_class\",None)\n",
    "        self.act = self.act_target = act_class(net_dims,state_dim,action_dim).to(self.device)\n",
    "        self.cri_class = self.cri_target = cri_class(net_dims,state_dim,action_dim).to(self.device) \\\n",
    "            if cri_class else self.act\n",
    "        \n",
    "        self.act_optimizer = optim.Adam(self.act.parameters(),self.learning_rate)\n",
    "        self.cri_optimizer = optim.Adam(self.cri_class.parameters(),self.learning_rate) \\\n",
    "            if cri_class else self.act_optimizer\n",
    "        self.loss = nn.SmoothL1Loss()\n",
    "    \n",
    "    @staticmethod\n",
    "    def optimizer_update(optimizer, object: torch.Tensor):\n",
    "        optimizer.zero_grad()\n",
    "        object.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    @staticmethod\n",
    "    def soft_update(target_net: nn.Module, current_net: nn.Module , tau: float):\n",
    "        for tar, cur in zip(target_net.parameters(), current_net.parameters()):\n",
    "            tar.data.copy_(cur.data * tau + tar.data * (1.0 - tau))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "design the agent\n",
    "$$\n",
    "\n",
    "\\delta \\hat{R_{\\theta}} = \\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=1}^{T_{n}} (R(\\epsilon^{n}) -b) \\delta \\log p_{\\theta}(a_{t}^{n}|s_{t}^{n}) \\\\\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (3219204704.py, line 78)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 78\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class AgentDQN(AgentBase):\n",
    "    def __init__(self, net_dims, state_dim: int, action_dim: int, args: Config = Config()):\n",
    "        self.act_class = getattr(self, \"act_class\", QNet)\n",
    "        self.cir_class = getattr(self,\"cri_class\", None)\n",
    "        AgentBase().__init__(self,net_dims, state_dim, action_dim,  args)\n",
    "        self.act_target = self.cir_target = deepcopy(self.act)\n",
    "        self.act.explore_rate = getattr(args, \"explore_rate\", 0.25)\n",
    "        ### 与环境交互\n",
    "    def explore_env(self,env,horizon_len: int, if_random: bool = False) :\n",
    "        ### 初始化\n",
    "        states = torch.zeros((horizon_len,self.state_dim),dtype=torch.float32).to(self.device)\n",
    "        actions = torch.zeros((horizon_len,1), dtype=torch.int32).to(self.device)\n",
    "        rewards = torch.ones(horizon_len, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.zeros(horizon_len, dtype=torch.bool).to(self.device)\n",
    "\n",
    "        ary_state = self.last_state\n",
    "\n",
    "        get_action = self.act.get_action\n",
    "        for i in range(horizon_len):\n",
    "            state = torch.as_tensor(ary_state, dtype=torch.float32, device=self.device)\n",
    "            ###greedy- eplison\n",
    "            if if_random:\n",
    "                action = torch.randint(self.action_dim, size = (1,))[0]\n",
    "            else:\n",
    "                action = get_action(state.unsqueeze(0))[0,0]\n",
    "            ary_action = action.detach().cpu().numpy()\n",
    "            ary_state, reward, done, _ = env.step(ary_action)\n",
    "            if done:\n",
    "                ary_state = env.reset()\n",
    "            states[i] = state\n",
    "            actions[i] = action\n",
    "            rewards[i] = reward\n",
    "            dones[i] = done\n",
    "        self.last_state = ary_state\n",
    "        rewards = (rewards * self.reward_scale).unsqueeze(1)\n",
    "        undones = (1.0 - dones.type(torch.float32)).unsqueeze(1)\n",
    "        return states, actions, rewards, undones\n",
    "\n",
    "    def get_obj_critic(self, buffer, batch_size: int):\n",
    "        with torch.no_grad():\n",
    "            state, action, reward, undone, next_state = buffer.sample(batch_size)\n",
    "            next_q = self.cri_target(next_state).max(dim=1, keepdim=True)[0]\n",
    "            \n",
    "            q_label = reward + undone * self.gamma * next_q\n",
    "        q_value = self.cri(state).gather(1, action.long())\n",
    "        obj_critic = self.criterion(q_value, q_label)\n",
    "        return obj_critic, q_value.mean()\n",
    "    \n",
    "    def update_net(self, buffer):\n",
    "        obj_critics = 0.0\n",
    "        q_values = 0.0\n",
    "\n",
    "        update_times = int(buffer.cursize * self.repeat_times / self.batch_size)\n",
    "        assert update_times >= 1\n",
    "        for i in range (update_times):\n",
    "            obj_critic, q_values = self.get_obj_critic(buffer, self.batch_size)\n",
    "            self.optimizer_update(self.cri_optimizer, obj_critic)\n",
    "            self.soft_update(self.cri_target, self.cri, self.soft_update_tau)\n",
    "            \n",
    "            obj_critics += obj_critic.item()\n",
    "            q_values += q_values.item()\n",
    "        return obj_critic / update_times, q_values / update_times\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size: int, state_dim: int, action_dim:int):\n",
    "        self.p = 0 # pointer\n",
    "        self.if_full = False\n",
    "        self.cur_size = 0\n",
    "        self.max_size = max_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.states = torch.empty((max_size, state_dim), dtype=torch.float32, device=self.device)\n",
    "        self.actions = torch.empty((max_size, action_dim), dtype=torch.float32, device=self.device)\n",
    "        self.rewards = torch.empty((max_size, 1), dtype=torch.float32, device=self.device)\n",
    "        self.undones = torch.empty((max_size, 1), dtype=torch.float32, device=self.device)\n",
    "    \n",
    "    def update(self, items):\n",
    "        states, actions, rewards, undones = items\n",
    "        p = self.p + rewards.shape[0]\n",
    "        if p > self.max_size:\n",
    "            self.if_full = True\n",
    "            p0 = self.p\n",
    "            p1 = self.max_size\n",
    "            p2 = self.max_size - self.p\n",
    "            p = p - self.max_size\n",
    "\n",
    "            self.states[p0:p1], self.states[0:p] = states[:p2], states[-p:]\n",
    "            \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
